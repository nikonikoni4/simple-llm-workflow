from simple_llm_playground.server.executor_manager import executor_manager
import os
from typing import Optional, Type, Callable
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
# 1. è®¾ç½® LLM å·¥å‚

# =============================================================================
# é€šç”¨ LLM å·¥å‚å‡½æ•°
# =============================================================================

def create_llm_factory(
    model: str = "qwen-plus-2025-12-01",
    api_key: Optional[str] = None,
    chat_model: Type[BaseChatModel] = ChatOpenAI, 
    enable_search: bool = False,
    enable_thinking: bool = False,
    **base_kwargs
) -> Callable[..., BaseChatModel]:
    """
    åˆ›å»º LLM å·¥å‚å‡½æ•°ï¼Œè¿”å›çš„ callback å¯åˆ›å»ºæ–°çš„ BaseChatModel å®ä¾‹

    é¢„å…ˆé…ç½® api_key å’Œ modelï¼Œè¿”å›çš„ callback åªéœ€è¦ä¼ å…¥ temperature ç­‰è¿è¡Œæ—¶å‚æ•°ã€‚

    Args:
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ qwen-plus-2025-12-01
        api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        enable_search: æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢
        enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
        **base_kwargs: å…¶ä»–é¢„é…ç½®çš„å‚æ•°

    Returns:
        è¿”å›ä¸€ä¸ªå‡½æ•°ï¼Œè°ƒç”¨æ—¶ä¼ å…¥ temperature ç­‰å‚æ•°å³å¯åˆ›å»ºæ–°å®ä¾‹

    Example:
        >>> factory = create_llm_factory(model="qwen-plus")
        >>> llm1 = factory(temperature=0.3)  # åˆ›å»ºä½æ¸©å®ä¾‹
        >>> llm2 = factory(temperature=0.9)  # åˆ›å»ºé«˜æ¸©å®ä¾‹
    """
    # è·å– API key
    if api_key is None:
        api_key = os.getenv("DASHSCOPE_API_KEY") or os.getenv("OPENAI_API_KEY")

    # é é…ç½®å‚æ•°
    base_config = {
        "model": model,
        "api_key": api_key,
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
    }
    
    if not issubclass(chat_model, BaseChatModel):
        raise ValueError(f"chat_model å¿…é¡»æ˜¯ BaseChatModel çš„å­ç±»ï¼Œä½†æ”¶åˆ°äº†: {chat_model}")

    if enable_search:
        base_config["enable_search"] = True
    if enable_thinking:
        base_config["enable_thinking"] = True

    base_config.update(base_kwargs)

    def callback(
        temperature: float = 0.7,
        **kwargs
    ) -> BaseChatModel:
        """åˆ›å»ºæ–°çš„ LLM å®ä¾‹"""
        config = {**base_config, "temperature": temperature}
        config.update(kwargs)
        return chat_model(**config)

    return callback

def setup_llm_factory():
    # api_key = "your_api_key"
    # model = "gpt-4o"
    # llm_factory = create_llm_factory(model,api_key,chat_model=ChatOpenAI)
    llm_factory = create_llm_factory(chat_model=ChatOpenAI)
    executor_manager.set_llm_factory(llm_factory)

 
# 2. è®¾ç½®å·¥å…·
# from your_path import ( tools )
def setup_test_tools():
    """è®¾ç½®æµ‹è¯•å·¥å…·ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰"""
    from langchain_core.tools import tool
    
    @tool
    def add(a: int, b: int) -> int:
        """Add two numbers"""
        return a + b
    
    @tool
    def multiply(a: int, b: int) -> int:
        """Multiply two numbers"""
        return a * b
    @tool
    def get_daily_stats(module: str = "all"):
        """
        è·å–ä»Šæ—¥ç»Ÿè®¡æ•°æ®ã€‚
        å‚æ•° module å¯é€‰å€¼:
        - 'all': è·å–å…¨éƒ¨æ•°æ®
        - 'active_distribution': 1. ç”µè„‘ä½¿ç”¨æ—¶é—´å æ¯”
        - 'behavior_stats': 2. è¡Œä¸ºæ•°æ®ç»Ÿè®¡
        - 'target_investment': 3. ç›®æ ‡æ—¶é—´æŠ•å…¥
        - 'task_status': 4. ä»Šæ—¥é‡ç‚¹ä¸ä»»åŠ¡
        - 'comparison': 5. ä¸å‰ä¸€å¤©å¯¹æ¯”
        """
        sections = {
            "active_distribution": """1. ç”µè„‘ä½¿ç”¨æ—¶é—´å æ¯”
    ç”µè„‘ä½¿ç”¨æ—¶é—´ï¼š
    - 0~1 : 0.0
    - 1~2 : 0.0
    - 2~3 : 0.0
    - 3~4 : 0.0
    - 4~5 : 0.0
    - 5~6 : 0.0
    - 6~7 : 0.0
    - 7~8 : 0.15
    - 8~9 : 0.42
    - 9~10 : 0.88
    - 10~11 : 0.95
    - 11~12 : 0.78
    - 12~13 : 0.22
    - 13~14 : 0.85
    - 14~15 : 0.91
    - 15~16 : 0.89
    - 16~17 : 0.84
    - 17~18 : 0.76
    - 18~19 : 0.45
    - 19~20 : 0.62
    - 20~21 : 0.88
    - 21~22 : 0.93
    - 22~23 : 0.81
    - 23~24 : 0.12""",
            
            "behavior_stats": """2. è¡Œä¸ºæ•°æ®ç»Ÿè®¡
    - æ—¶æ®µ1ï¼ˆ2026-02-14 00:00:00 è‡³ 2026-02-14 05:59:59ï¼‰
        - åˆ†ç±»å æ¯”:
        - ç”µè„‘ç©ºé—²æ—¶é—´: 5å°æ—¶59åˆ†é’Ÿï¼ˆ100.0%ï¼‰
    - æ—¶æ®µ2ï¼ˆ2026-02-14 05:59:59 è‡³ 2026-02-14 11:59:59ï¼‰
        - åˆ†ç±»å æ¯”:
        - å·¥ä½œ/å­¦ä¹ : 4å°æ—¶12åˆ†é’Ÿï¼ˆ70.0%ï¼‰
            - ç¼–ç¨‹: 3å°æ—¶25åˆ†é’Ÿï¼ˆ56.9%ï¼‰
            - æ–‡æ¡£æ’°å†™: 35åˆ†é’Ÿï¼ˆ9.7%ï¼‰
            - æ²Ÿé€š: 12åˆ†é’Ÿï¼ˆ3.4%ï¼‰
        - ç”µè„‘ç©ºé—²æ—¶é—´: 1å°æ—¶28åˆ†é’Ÿï¼ˆ24.4%ï¼‰
        - å…¶ä»–: 20åˆ†é’Ÿï¼ˆ5.6%ï¼‰
        - ä¸»è¦æ´»åŠ¨è®°å½•:
        - nebula-core - architecture - design_doc.mdï¼ˆvscodeï¼‰: 45åˆ†é’Ÿ
        - nebula-core - engine - optimizer.pyï¼ˆvscodeï¼‰: 32åˆ†é’Ÿ
        - nebula-explorerï¼ˆmsedgeï¼‰: 18åˆ†é’Ÿ
        - terminal - build engineï¼ˆpowershellï¼‰: 12åˆ†é’Ÿ
        - slack - team syncï¼ˆslackï¼‰: 10åˆ†é’Ÿ
    - æ—¶æ®µ3ï¼ˆ2026-02-14 11:59:59 è‡³ 2026-02-14 17:59:59ï¼‰
        - åˆ†ç±»å æ¯”:
        - å·¥ä½œ/å­¦ä¹ : 3å°æ—¶45åˆ†é’Ÿï¼ˆ62.5%ï¼‰
            - ç¼–ç¨‹: 3å°æ—¶10åˆ†é’Ÿï¼ˆ52.8%ï¼‰
            - è°ƒè¯•: 25åˆ†é’Ÿï¼ˆ6.9%ï¼‰
            - è®¡åˆ’: 10åˆ†é’Ÿï¼ˆ2.8%ï¼‰
        - ç”µè„‘ç©ºé—²æ—¶é—´: 1å°æ—¶35åˆ†é’Ÿï¼ˆ26.4%ï¼‰
        - å¨±ä¹: 40åˆ†é’Ÿï¼ˆ11.1%ï¼‰
            - éŸ³ä¹: 40åˆ†é’Ÿï¼ˆ11.1%ï¼‰
        - ä¸»è¦æ´»åŠ¨è®°å½•:
        - nebula-core - tests - test_optimizer.pyï¼ˆvscodeï¼‰: 55åˆ†é’Ÿ
        - stackoverflow - python profile optimizationï¼ˆmsedgeï¼‰: 20åˆ†é’Ÿ
        - nebula-core - engine - pipeline.pyï¼ˆvscodeï¼‰: 15åˆ†é’Ÿ
        - spotifyï¼ˆspotifyï¼‰: 40åˆ†é’Ÿ
        - jira - sprint planningï¼ˆmsedgeï¼‰: 10åˆ†é’Ÿ
    - æ—¶æ®µ4ï¼ˆ2026-02-14 17:59:59 è‡³ 2026-02-14 23:59:59ï¼‰
        - åˆ†ç±»å æ¯”:
        - å¨±ä¹: 3å°æ—¶15åˆ†é’Ÿï¼ˆ54.2%ï¼‰
            - æ¸¸æˆ: 2å°æ—¶45åˆ†é’Ÿï¼ˆ45.8%ï¼‰
            - ç¤¾äº¤: 30åˆ†é’Ÿï¼ˆ8.4%ï¼‰
        - å·¥ä½œ/å­¦ä¹ : 1å°æ—¶10åˆ†é’Ÿï¼ˆ19.4%ï¼‰
            - ç¼–ç¨‹: 1å°æ—¶10åˆ†é’Ÿï¼ˆ19.4%ï¼‰
        - ç”µè„‘ç©ºé—²æ—¶é—´: 1å°æ—¶35åˆ†é’Ÿï¼ˆ26.4%ï¼‰
        - ä¸»è¦æ´»åŠ¨è®°å½•:
        - Cyberpunk 2077ï¼ˆgame_exeï¼‰: 2å°æ—¶20åˆ†é’Ÿ
        - Discord - gaming communityï¼ˆdiscordï¼‰: 30åˆ†é’Ÿ
        - nebula-core - hotfix - bug_fix.pyï¼ˆvscodeï¼‰: 25åˆ†é’Ÿ
        - Youtube - tech reviewsï¼ˆmsedgeï¼‰: 25åˆ†é’Ÿ""",

            "target_investment": """3. ç›®æ ‡æ—¶é—´æŠ•å…¥
    - å®ŒæˆNebulaæ ¸å¿ƒå¼•æ“: 8å°æ—¶47åˆ†é’Ÿ""",

            "task_status": """4. ä»Šæ—¥é‡ç‚¹ä¸ä»»åŠ¡
    - focus : 1. ä¼˜åŒ–æŸ¥è¯¢æ‰§è¡Œå™¨æ€§èƒ½
    2. ç¼–å†™é›†æˆæµ‹è¯•æŠ¥å‘Š
    3. é‡æ„æ—¥å¿—ç®¡ç†æ¨¡å—
    - todos: 85%
    1. ä¿®å¤å†…å­˜æ³„éœ²é—®é¢˜ completed
    2. å®ç°æŸ¥è¯¢ç¼“å­˜æœºåˆ¶ completed
    3. è¡¥å……æ–‡æ¡£æ³¨é‡Š in_progress""",

            "comparison": """5. ä¸å‰ä¸€å¤©å¯¹æ¯”
    ### åˆ†ç±»æ—¶é—´å˜åŒ–
    | åˆ†ç±» | ä¸Šå‘¨æœŸ | æœ¬å‘¨æœŸ | å˜åŒ– |
    |------|--------|--------|------|
    | å·¥ä½œ/å­¦ä¹  | 7.5h | 9.1h | +21.3% |
    | å¨±ä¹ | 2.5h | 3.9h | +56.0% |
    | å…¶ä»– | 1.8h | 1.0h | -44.4% |

    ### ç›®æ ‡æŠ•å…¥å˜åŒ–
    - å®ŒæˆNebulaæ ¸å¿ƒå¼•æ“: 6.8h â†’ 8.8h (+2.0h)"""
        }

        if module == "all":
            return "\n\n".join(sections.values())
        
        return sections.get(module, f"é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å— '{module}'ã€‚å¯ç”¨é€‰é¡¹: {list(sections.keys())}")

    executor_manager.register_tool("add", add)
    executor_manager.register_tool("multiply", multiply)
    executor_manager.register_tool("get_daily_stats", get_daily_stats)



# 3. è¿è¡Œåç«¯æœåŠ¡
if __name__ == "__main__":
    import uvicorn
    from simple_llm_playground.server.backend_api import app
    from simple_llm_playground.server.executor_manager import executor_manager
    from simple_llm_playground import config
    # 1. è®¾ç½® LLM å·¥å‚
    setup_llm_factory()

    # 2. è®¾ç½®å·¥å…·
    setup_test_tools()

    # 3. è¿è¡Œåç«¯æœåŠ¡
    # è·å–ç«¯å£é…ç½®ï¼Œå¦‚æœ config.py ä¸­æ²¡æœ‰å®šä¹‰åˆ™ä½¿ç”¨é»˜è®¤å€¼ 8001
    port = getattr(config, "BACKEND_PORT", 8001)
    
    print(f"ğŸš€ Starting Backend Server from main.py on port {port}...")
    print(f"âœ… Tools registered: {list(executor_manager._tools_registry.keys())}")
    
    uvicorn.run(app, host="0.0.0.0", port=port)