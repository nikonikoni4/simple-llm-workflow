# FastAPI åç«¯æœåŠ¡
# æä¾› RESTful API ç”¨äºå‰ç«¯ä¸ AsyncExecutor äº¤äº’
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
import os
from simple_llm_playground.server.executor_manager import executor_manager
from simple_llm_playground.schemas import (
    ExecutionPlan,
    InitExecutorRequest, InitExecutorResponse,
    StepExecutorRequest, StepExecutorResponse,
    ExecutorStatusResponse, ExecutionResultResponse,
    NodeContextResponse,
    HealthCheckResponse, ToolInfo, ToolListResponse,
    TerminateExecutorResponse, ListExecutorsResponse, ExecutorInfo
)




# =============================================================================
# FastAPI åº”ç”¨
# =============================================================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶çš„åˆå§‹åŒ–
    print("ğŸš€ Backend API starting...")
    
    # è¿™é‡Œå¯ä»¥åŠ è½½é»˜è®¤å·¥å…·å’Œ LLM å·¥å‚
    # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®é¡¹ç›®é…ç½®æ¥è®¾ç½®
    
    # åœ¨ä½¿ç”¨ uvicorn è¿è¡Œæ—¶è®¾ç½®æµ‹è¯•å·¥å…·
    
    # setup_test_tools()
    # åŒæ—¶å°è¯•ä» test ç›®å½•åŠ è½½ get_daily_stats
    # try:
    #     # å¦‚æœ test ç›®å½•ä¸åœ¨ path ä¸­ï¼Œåˆ™å°†å…¶æ·»åŠ è¿›å»
    #     test_dir = os.path.join(parent_dir, "test")
    #     if test_dir not in sys.path:
    #         sys.path.insert(0, test_dir)
        
    #     from test_fuction.get_daily_stats import get_daily_stats
    #     executor_manager.register_tool("get_daily_stats", get_daily_stats)
    #     print("âœ… Registered tool: get_daily_stats")
    # except Exception as e:
    #     print(f"âš ï¸ Warning: Could not load get_daily_stats tool: {e}")
    
    # è®¾ç½® LLM å·¥å‚ (ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼)
    setup_llm_factory()
    
    yield
    
    # å…³é—­æ—¶çš„æ¸…ç†
    print("ğŸ›‘ Backend API shutting down...")
    executor_manager.executors.clear()


app = FastAPI(
    title="Simple LLM Playground API",
    description="Backend API for LLM Executor debugging and visualization",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =============================================================================
# API ç«¯ç‚¹
# =============================================================================

@app.get("/", response_model=HealthCheckResponse)
async def root():
    """æ ¹è·¯å¾„ - å¥åº·æ£€æŸ¥"""
    return HealthCheckResponse(
        status="running",
        message="Simple LLM Playground API v1.0.0"
    )


@app.get("/api/tools", response_model=ToolListResponse)
async def list_tools():
    """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„å·¥å…·"""
    tools = []
    for name, tool in executor_manager._tools_registry.items():
        tool_data = {
            "name": name,
            "description": getattr(tool, 'description', 'No description'),
        }
        
        # å°è¯•ä» langchain å·¥å…·ä¸­æå–å‚æ•°ä¿¡æ¯
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå…·æœ‰ args_schema çš„ langchain å·¥å…·
            if hasattr(tool, 'args_schema'):
                schema = tool.args_schema
                if schema:
                    # ä» pydantic æ¨¡å‹è·å–å­—æ®µä¿¡æ¯
                    tool_data["parameters"] = {}
                    if hasattr(schema, 'model_fields'):
                        for field_name, field_info in schema.model_fields.items():
                            tool_data["parameters"][field_name] = {
                                "type": str(field_info.annotation),
                                "required": field_info.is_required(),
                                "description": field_info.description or ""
                            }
            
            # åŒæ—¶å°è¯•ä»å‡½æ•°ç­¾åä¸­è·å–
            if hasattr(tool, 'func'):
                import inspect
                sig = inspect.signature(tool.func)
                if "parameters" not in tool_data:
                    tool_data["parameters"] = {}
                
                for param_name, param in sig.parameters.items():
                    if param_name not in tool_data["parameters"]:
                        tool_data["parameters"][param_name] = {
                            "type": str(param.annotation) if param.annotation != inspect.Parameter.empty else "Any",
                            "required": param.default == inspect.Parameter.empty,
                            "description": ""
                        }
        except Exception as e:
            # å¦‚æœæå–å¤±è´¥ï¼Œåˆ™è·³è¿‡å‚æ•°æå–
            print(f"Warning: Could not extract parameters for tool {name}: {e}")
        
        tools.append(ToolInfo(**tool_data))
    
    return ToolListResponse(tools=tools)



@app.post("/api/executor/init", response_model=InitExecutorResponse)
async def init_executor(request: InitExecutorRequest):
    """
    åˆå§‹åŒ–æ‰§è¡Œå™¨

    åˆ›å»ºä¸€ä¸ªæ–°çš„ AsyncExecutor å®ä¾‹ï¼Œå‡†å¤‡æ‰§è¡Œè®¡åˆ’
    """
    try:
        # è§£æ ExecutionPlan
        plan = ExecutionPlan(**request.plan)
        if request.default_tool_limit is None:
            request.default_tool_limit = 1
        # åˆ›å»ºæ‰§è¡Œå™¨
        executor_id = executor_manager.create_executor(
            plan=plan,
            default_tools_limit=request.default_tool_limit # å½“è¿™ä¸ªæ˜¯Noneæ—¶ï¼Œå¯¼è‡´åé¢ä¼šæŠ¥é”™
        )
        
        return InitExecutorResponse(
            executor_id=executor_id,
            status="initialized",
            node_count=len(plan.nodes),
            message=f"Executor initialized with {len(plan.nodes)} nodes"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/api/executor/{executor_id}/run", response_model=ExecutionResultResponse)
async def run_executor(executor_id: str, background_tasks: BackgroundTasks):
    """
    è¿è¡Œæ‰§è¡Œå™¨ï¼ˆæ‰§è¡Œæ•´ä¸ªè®¡åˆ’ï¼‰
    
    åœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œï¼Œç«‹å³è¿”å›
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    # æ›´æ–°çŠ¶æ€
    executor_manager.executor_status[executor_id] = "running"
    
    async def execute_in_background():
        try:
            result = await executor.execute()
            executor_manager.executor_status[executor_id] = "completed"
        except Exception as e:
            executor_manager.executor_status[executor_id] = f"failed: {str(e)}"
    
    # æ·»åŠ åå°ä»»åŠ¡
    background_tasks.add_task(execute_in_background)
    
    return ExecutionResultResponse(
        executor_id=executor_id,
        status="running",
        content=None,
        tokens_usage=executor.tokens_usage,
        message="æ‰§è¡Œå·²åœ¨åå°å¼€å§‹"
    )


@app.post("/api/executor/{executor_id}/run-sync", response_model=ExecutionResultResponse)
async def run_executor_sync(executor_id: str):
    """
    åŒæ­¥è¿è¡Œæ‰§è¡Œå™¨ï¼ˆç­‰å¾…æ‰§è¡Œå®Œæˆï¼‰
    
    ç›´æ¥æ‰§è¡Œå¹¶è¿”å›ç»“æœï¼Œé€‚ç”¨äºéœ€è¦ç«‹å³è·å–ç»“æœçš„åœºæ™¯
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    # æ›´æ–°çŠ¶æ€
    executor_manager.executor_status[executor_id] = "running"
    
    try:
        result = await executor.execute()
        executor_manager.executor_status[executor_id] = "completed"
        
        return ExecutionResultResponse(
            executor_id=executor_id,
            status="completed",
            content=result.get("content"),
            tokens_usage=result.get("tokens_usage", {}),
            message="Execution completed"
        )
    except Exception as e:
        executor_manager.executor_status[executor_id] = "failed"
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/executor/{executor_id}/step", response_model=StepExecutorResponse)
async def step_executor(executor_id: str, request: StepExecutorRequest = None):
    """
    å•æ­¥æ‰§è¡Œ
    
    æ‰§è¡Œä¸‹ä¸€ä¸ªå¾…æ‰§è¡Œçš„èŠ‚ç‚¹ï¼Œè¿”å›èŠ‚ç‚¹ä¸Šä¸‹æ–‡
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    try:
        # æ‰§è¡Œå•æ­¥
        context = await executor.execute_step()
        
        if context is None:
            return StepExecutorResponse(
                status="completed",
                message="All nodes have been executed",
                node_context=None,
                progress=executor.get_execution_progress()
            )
        
        return StepExecutorResponse(
            status="success",
            message=f"Node {context.node_id} executed",
            node_context=context.model_dump(),
            progress=executor.get_execution_progress()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/executor/{executor_id}/status", response_model=ExecutorStatusResponse)
async def get_executor_status(executor_id: str):
    """
    è·å–æ‰§è¡Œå™¨çŠ¶æ€
    
    è¿”å›æ•´ä½“çŠ¶æ€å’Œæ‰€æœ‰èŠ‚ç‚¹çš„æ‰§è¡ŒçŠ¶æ€
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    overall_status = executor_manager.executor_status.get(executor_id, "unknown")
    
    return ExecutorStatusResponse(
        executor_id=executor_id,
        overall_status=overall_status,
        progress=executor.get_execution_progress(),
        node_states=[s.model_dump() for s in executor.get_all_node_states()]
    )


@app.get("/api/executor/{executor_id}/nodes/{node_id}/context", response_model=NodeContextResponse)
async def get_node_context(executor_id: str, node_id: int):
    """
    è·å–èŠ‚ç‚¹ä¸Šä¸‹æ–‡
    
    è¿”å›æŒ‡å®šèŠ‚ç‚¹çš„è¯¦ç»†æ‰§è¡Œä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    context = executor.get_node_context(node_id)
    if not context:
        raise HTTPException(status_code=404, detail=f"Context for node {node_id} not found")
    
    return NodeContextResponse(**context.model_dump())


@app.get("/api/executor/{executor_id}/messages")
async def get_executor_messages(executor_id: str, thread_id: str = None):
    """
    è·å–æ‰§è¡Œå™¨çš„æ¶ˆæ¯
    
    å¯é€‰æŒ‡å®š thread_id è·å–ç‰¹å®šçº¿ç¨‹çš„æ¶ˆæ¯
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    if thread_id:
        messages = executor._get_thread_messages(thread_id)
        return {
            "thread_id": thread_id,
            "messages": executor._serialize_messages(messages)
        }
    else:
        # è¿”å›æ‰€æœ‰çº¿ç¨‹çš„æ¶ˆæ¯
        all_messages = {}
        for tid in executor.context["messages"]:
            all_messages[tid] = executor._serialize_messages(
                executor.context["messages"][tid]
            )
        return {"threads": all_messages}


@app.delete("/api/executor/{executor_id}", response_model=TerminateExecutorResponse)
async def terminate_executor(executor_id: str):
    """
    ç»ˆæ­¢å¹¶åˆ é™¤æ‰§è¡Œå™¨
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    executor_manager.remove_executor(executor_id)
    
    return TerminateExecutorResponse(
        status="terminated",
        message=f"Executor {executor_id} has been terminated"
    )


@app.get("/api/executors", response_model=ListExecutorsResponse)
async def list_executors():
    """
    åˆ—å‡ºæ‰€æœ‰æ‰§è¡Œå™¨
    """
    from datetime import datetime
    executors = []
    for eid, executor in executor_manager.executors.items():
        # è·å–å¯åŠ¨æ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
        start_time = executor_manager.executor_start_times.get(eid, datetime.now().isoformat())
        executors.append(ExecutorInfo(
            executor_id=eid,
            start_time=start_time if isinstance(start_time, str) else start_time.isoformat(),
            status=executor_manager.executor_status.get(eid, "unknown")
        ))
    return ListExecutorsResponse(executors=executors)


# =============================================================================
# å·¥å…·æ³¨å†Œ APIï¼ˆç”¨äºåŠ¨æ€æ³¨å†Œå·¥å…·ï¼‰
# =============================================================================

@app.post("/api/tools/register")
async def register_tool_endpoint(
    tool_name: str,
    tool_module: str,
    tool_function: str,
    limit: int = 10
):
    """
    åŠ¨æ€æ³¨å†Œå·¥å…·
    
    ä»æŒ‡å®šæ¨¡å—å¯¼å…¥å·¥å…·å‡½æ•°å¹¶æ³¨å†Œ
    """
    try:
        import importlib
        module = importlib.import_module(tool_module)
        tool_func = getattr(module, tool_function)
        
        executor_manager.register_tool(tool_name, tool_func)
        
        return {
            "status": "success",
            "message": f"Tool '{tool_name}' registered successfully"
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# =============================================================================
# ç”¨äºæµ‹è¯•çš„è¾…åŠ©å‡½æ•°
# =============================================================================
def setup_test_tools():
    """è®¾ç½®æµ‹è¯•å·¥å…·ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰"""
    from langchain_core.tools import tool
    
    @tool
    def add(a: int, b: int) -> int:
        """Add two numbers"""
        return a + b
    
    @tool
    def multiply(a: int, b: int) -> int:
        """Multiply two numbers"""
        return a * b
    @tool
    def get_daily_stats(module: str = "all"):
        """
        è·å–ä»Šæ—¥ç»Ÿè®¡æ•°æ®ã€‚
        å‚æ•° module å¯é€‰å€¼:
        - 'all': è·å–å…¨éƒ¨æ•°æ®
        - 'active_distribution': 1. ç”µè„‘ä½¿ç”¨æ—¶é—´å æ¯”
        - 'behavior_stats': 2. è¡Œä¸ºæ•°æ®ç»Ÿè®¡
        - 'target_investment': 3. ç›®æ ‡æ—¶é—´æŠ•å…¥
        - 'task_status': 4. ä»Šæ—¥é‡ç‚¹ä¸ä»»åŠ¡
        - 'comparison': 5. ä¸å‰ä¸€å¤©å¯¹æ¯”
        """
        sections = {
            "active_distribution": """1. ç”µè„‘ä½¿ç”¨æ—¶é—´å æ¯”
    ç”µè„‘ä½¿ç”¨æ—¶é—´ï¼š
    - 0~1 : 0.0
    - 1~2 : 0.0
    - 2~3 : 0.0
    - 3~4 : 0.0
    - 4~5 : 0.0
    - 5~6 : 0.0
    - 6~7 : 0.0
    - 7~8 : 0.15
    - 8~9 : 0.42
    - 9~10 : 0.88
    - 10~11 : 0.95
    - 11~12 : 0.78
    - 12~13 : 0.22
    - 13~14 : 0.85
    - 14~15 : 0.91
    - 15~16 : 0.89
    - 16~17 : 0.84
    - 17~18 : 0.76
    - 18~19 : 0.45
    - 19~20 : 0.62
    - 20~21 : 0.88
    - 21~22 : 0.93
    - 22~23 : 0.81
    - 23~24 : 0.12""",
            
            "behavior_stats": """2. è¡Œä¸ºæ•°æ®ç»Ÿè®¡
    - æ—¶æ®µ1ï¼ˆ2026-02-14 00:00:00 è‡³ 2026-02-14 05:59:59ï¼‰
        - åˆ†ç±»å æ¯”:
        - ç”µè„‘ç©ºé—²æ—¶é—´: 5å°æ—¶59åˆ†é’Ÿï¼ˆ100.0%ï¼‰
    - æ—¶æ®µ2ï¼ˆ2026-02-14 05:59:59 è‡³ 2026-02-14 11:59:59ï¼‰
        - åˆ†ç±»å æ¯”:
        - å·¥ä½œ/å­¦ä¹ : 4å°æ—¶12åˆ†é’Ÿï¼ˆ70.0%ï¼‰
            - ç¼–ç¨‹: 3å°æ—¶25åˆ†é’Ÿï¼ˆ56.9%ï¼‰
            - æ–‡æ¡£æ’°å†™: 35åˆ†é’Ÿï¼ˆ9.7%ï¼‰
            - æ²Ÿé€š: 12åˆ†é’Ÿï¼ˆ3.4%ï¼‰
        - ç”µè„‘ç©ºé—²æ—¶é—´: 1å°æ—¶28åˆ†é’Ÿï¼ˆ24.4%ï¼‰
        - å…¶ä»–: 20åˆ†é’Ÿï¼ˆ5.6%ï¼‰
        - ä¸»è¦æ´»åŠ¨è®°å½•:
        - nebula-core - architecture - design_doc.mdï¼ˆvscodeï¼‰: 45åˆ†é’Ÿ
        - nebula-core - engine - optimizer.pyï¼ˆvscodeï¼‰: 32åˆ†é’Ÿ
        - nebula-explorerï¼ˆmsedgeï¼‰: 18åˆ†é’Ÿ
        - terminal - build engineï¼ˆpowershellï¼‰: 12åˆ†é’Ÿ
        - slack - team syncï¼ˆslackï¼‰: 10åˆ†é’Ÿ
    - æ—¶æ®µ3ï¼ˆ2026-02-14 11:59:59 è‡³ 2026-02-14 17:59:59ï¼‰
        - åˆ†ç±»å æ¯”:
        - å·¥ä½œ/å­¦ä¹ : 3å°æ—¶45åˆ†é’Ÿï¼ˆ62.5%ï¼‰
            - ç¼–ç¨‹: 3å°æ—¶10åˆ†é’Ÿï¼ˆ52.8%ï¼‰
            - è°ƒè¯•: 25åˆ†é’Ÿï¼ˆ6.9%ï¼‰
            - è®¡åˆ’: 10åˆ†é’Ÿï¼ˆ2.8%ï¼‰
        - ç”µè„‘ç©ºé—²æ—¶é—´: 1å°æ—¶35åˆ†é’Ÿï¼ˆ26.4%ï¼‰
        - å¨±ä¹: 40åˆ†é’Ÿï¼ˆ11.1%ï¼‰
            - éŸ³ä¹: 40åˆ†é’Ÿï¼ˆ11.1%ï¼‰
        - ä¸»è¦æ´»åŠ¨è®°å½•:
        - nebula-core - tests - test_optimizer.pyï¼ˆvscodeï¼‰: 55åˆ†é’Ÿ
        - stackoverflow - python profile optimizationï¼ˆmsedgeï¼‰: 20åˆ†é’Ÿ
        - nebula-core - engine - pipeline.pyï¼ˆvscodeï¼‰: 15åˆ†é’Ÿ
        - spotifyï¼ˆspotifyï¼‰: 40åˆ†é’Ÿ
        - jira - sprint planningï¼ˆmsedgeï¼‰: 10åˆ†é’Ÿ
    - æ—¶æ®µ4ï¼ˆ2026-02-14 17:59:59 è‡³ 2026-02-14 23:59:59ï¼‰
        - åˆ†ç±»å æ¯”:
        - å¨±ä¹: 3å°æ—¶15åˆ†é’Ÿï¼ˆ54.2%ï¼‰
            - æ¸¸æˆ: 2å°æ—¶45åˆ†é’Ÿï¼ˆ45.8%ï¼‰
            - ç¤¾äº¤: 30åˆ†é’Ÿï¼ˆ8.4%ï¼‰
        - å·¥ä½œ/å­¦ä¹ : 1å°æ—¶10åˆ†é’Ÿï¼ˆ19.4%ï¼‰
            - ç¼–ç¨‹: 1å°æ—¶10åˆ†é’Ÿï¼ˆ19.4%ï¼‰
        - ç”µè„‘ç©ºé—²æ—¶é—´: 1å°æ—¶35åˆ†é’Ÿï¼ˆ26.4%ï¼‰
        - ä¸»è¦æ´»åŠ¨è®°å½•:
        - Cyberpunk 2077ï¼ˆgame_exeï¼‰: 2å°æ—¶20åˆ†é’Ÿ
        - Discord - gaming communityï¼ˆdiscordï¼‰: 30åˆ†é’Ÿ
        - nebula-core - hotfix - bug_fix.pyï¼ˆvscodeï¼‰: 25åˆ†é’Ÿ
        - Youtube - tech reviewsï¼ˆmsedgeï¼‰: 25åˆ†é’Ÿ""",

            "target_investment": """3. ç›®æ ‡æ—¶é—´æŠ•å…¥
    - å®ŒæˆNebulaæ ¸å¿ƒå¼•æ“: 8å°æ—¶47åˆ†é’Ÿ""",

            "task_status": """4. ä»Šæ—¥é‡ç‚¹ä¸ä»»åŠ¡
    - focus : 1. ä¼˜åŒ–æŸ¥è¯¢æ‰§è¡Œå™¨æ€§èƒ½
    2. ç¼–å†™é›†æˆæµ‹è¯•æŠ¥å‘Š
    3. é‡æ„æ—¥å¿—ç®¡ç†æ¨¡å—
    - todos: 85%
    1. ä¿®å¤å†…å­˜æ³„éœ²é—®é¢˜ completed
    2. å®ç°æŸ¥è¯¢ç¼“å­˜æœºåˆ¶ completed
    3. è¡¥å……æ–‡æ¡£æ³¨é‡Š in_progress""",

            "comparison": """5. ä¸å‰ä¸€å¤©å¯¹æ¯”
    ### åˆ†ç±»æ—¶é—´å˜åŒ–
    | åˆ†ç±» | ä¸Šå‘¨æœŸ | æœ¬å‘¨æœŸ | å˜åŒ– |
    |------|--------|--------|------|
    | å·¥ä½œ/å­¦ä¹  | 7.5h | 9.1h | +21.3% |
    | å¨±ä¹ | 2.5h | 3.9h | +56.0% |
    | å…¶ä»– | 1.8h | 1.0h | -44.4% |

    ### ç›®æ ‡æŠ•å…¥å˜åŒ–
    - å®ŒæˆNebulaæ ¸å¿ƒå¼•æ“: 6.8h â†’ 8.8h (+2.0h)"""
        }

        if module == "all":
            return "\n\n".join(sections.values())
        
        return sections.get(module, f"é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å— '{module}'ã€‚å¯ç”¨é€‰é¡¹: {list(sections.keys())}")

    executor_manager.register_tool("add", add)
    executor_manager.register_tool("multiply", multiply)
    # executor_manager.register_tool("get_daily_stats", get_daily_stats)


def setup_llm_factory(
    api_key: str = None,
    model: str = "qwen-plus-2025-12-01",
    api_base: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
    **kwargs
):
    """
    è®¾ç½® LLM å·¥å‚å‡½æ•°

    æ”¯æŒé˜¿é‡Œäº‘ DashScope API (é€šä¹‰åƒé—®) å’Œ OpenAI API

    Args:
        api_key: APIå¯†é’¥ (DashScope API Key æˆ– OpenAI API Key)ã€‚å¦‚æœä¸ä¼ ï¼Œå°è¯•ä»ç¯å¢ƒè¯»å–ã€‚
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ "qwen-plus"
            - é€šä¹‰åƒé—®: "qwen-plus", "qwen-max", "qwen-turbo" ç­‰
            - OpenAI: "gpt-4", "gpt-3.5-turbo" ç­‰
        api_base: APIåŸºç¡€URL
            - é˜¿é‡Œäº‘: "https://dashscope.aliyuncs.com/compatible-mode/v1"
            - OpenAI: "https://api.openai.com/v1" (é»˜è®¤)
        **kwargs: å…¶ä»–å‚æ•°å¦‚ temperature, top_p ç­‰
    """
    # å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å– API Key
    if not api_key:
        api_key = os.getenv("DASHSCOPE_API_KEY") or os.getenv("OPENAI_API_KEY")

    if not api_key:
        print("âš ï¸ Warning: No API key found. Please set DASHSCOPE_API_KEY or OPENAI_API_KEY environment variable.")

    # ä½¿ç”¨ lambda æ•è·æ‰€æœ‰å‚æ•°ï¼Œç¡®ä¿é—­åŒ…æ­£ç¡®æ•è·å˜é‡
    factory = lambda: _create_llm_instance(
        model=model,
        api_key=api_key,
        api_base=api_base,
        **kwargs
    )

    executor_manager.set_llm_factory(factory)


def _create_llm_instance(
    model: str,
    api_key: str,
    api_base: str,
    **kwargs
):
    """
    åˆ›å»º LLM å®ä¾‹çš„è¾…åŠ©å‡½æ•°

    Args:
        model: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        api_base: APIåŸºç¡€URL
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        ChatOpenAI å®ä¾‹
    """
    try:
        from langchain_openai import ChatOpenAI

        # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼ï¼Œæ”¯æŒé˜¿é‡Œäº‘ DashScope å’Œ OpenAI
        return ChatOpenAI(
            model=model,
            openai_api_key=api_key,
            openai_api_base=api_base,
            temperature=kwargs.get('temperature', 0.7),
            top_p=kwargs.get('top_p', 0.9)
        )
    except ImportError:
        raise ValueError("langchain_openai not installed. Run: pip install langchain-openai")


# =============================================================================
# å¯åŠ¨å…¥å£
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    
    # 1. è®¾ç½® LLM å·¥å‚ (ä¼˜å…ˆ)
    setup_llm_factory()
    
    # 2. è®¾ç½®æµ‹è¯•å·¥å…·
    setup_test_tools()
    # æµ‹è¯• LLM  é“¾æ¥
    # llm = executor_manager._llm_factory()
    # print(llm.invoke("Hello, how are you?"))
    # å°è¯•ä» main.py å¯¼å…¥å…¨å±€é…ç½®
    try:
        import config
        port = getattr(config, "BACKEND_PORT", 8001)
        print(f"âš™ï¸  Loaded configuration from main.py: Port {port}")
    except ImportError:
        port = 8001
        print("âš ï¸  Warning: Could not import BACKEND_PORT from main.py, using default 8001")

    print("ğŸš€ Starting Simple LLM Playground API...")
    print(f"ğŸ“ API docs available at: http://localhost:{port}/docs")

    uvicorn.run(app, host="0.0.0.0", port=port)
