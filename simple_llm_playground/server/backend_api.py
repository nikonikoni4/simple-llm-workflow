# FastAPI åç«¯æœåŠ¡
# æä¾› RESTful API ç”¨äºå‰ç«¯ä¸ AsyncExecutor äº¤äº’


from typing import Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ° path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)


from .data_driving_schemas import ExecutionPlan
from .executor_manager import executor_manager


# =============================================================================
# è¯·æ±‚/å“åº”æ¨¡å‹
# =============================================================================
class ModelConfig(BaseModel):
    """æ¨¡å‹é…ç½®"""
    api_key: Optional[str] = None
    enable_search: bool = False
    enable_thinking: bool = False
    temperature: float = 0.7
    top_p: float = 0.9


class InitExecutorRequest(BaseModel):
    """åˆå§‹åŒ–æ‰§è¡Œå™¨è¯·æ±‚"""
    plan: dict  # ExecutionPlan çš„å­—å…¸å½¢å¼
    user_message: str
    default_tool_limit: Optional[int] = 1  # é»˜è®¤å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶
    llm_config: Optional[ModelConfig] = None  # é‡å‘½åé¿å…ä¸ Pydantic ä¿ç•™å­—æ®µå†²çª


class InitExecutorResponse(BaseModel):
    """åˆå§‹åŒ–æ‰§è¡Œå™¨å“åº”"""
    executor_id: str
    status: str
    node_count: int
    message: str


class StepExecutorRequest(BaseModel):
    """å•æ­¥æ‰§è¡Œè¯·æ±‚"""
    node_id: Optional[int] = None  # å¯é€‰ï¼Œä¸æŒ‡å®šåˆ™æ‰§è¡Œä¸‹ä¸€ä¸ª


class ExecutorStatusResponse(BaseModel):
    """æ‰§è¡Œå™¨çŠ¶æ€å“åº”"""
    executor_id: str
    overall_status: str
    progress: dict
    node_states: list[dict]


class NodeContextResponse(BaseModel):
    """èŠ‚ç‚¹ä¸Šä¸‹æ–‡å“åº”"""
    node_id: int
    node_name: str
    thread_id: str
    thread_messages_before: list[dict]
    thread_messages_after: list[dict]
    llm_input: str
    llm_output: str
    tool_calls: list[dict]
    data_out_content: Optional[str]


class ExecutionResultResponse(BaseModel):
    """æ‰§è¡Œç»“æœå“åº”"""
    executor_id: str
    status: str
    content: Optional[str]
    tokens_usage: dict
    message: str





# =============================================================================
# FastAPI åº”ç”¨
# =============================================================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶çš„åˆå§‹åŒ–
    print("ğŸš€ Backend API starting...")
    
    # è¿™é‡Œå¯ä»¥åŠ è½½é»˜è®¤å·¥å…·å’Œ LLM å·¥å‚
    # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®é¡¹ç›®é…ç½®æ¥è®¾ç½®
    
    # Setup test tools when running with uvicorn
    
    # setup_test_tools()
    # Also try to load get_daily_stats from test directory
    # try:
    #     # Add test directory to path if not already there
    #     test_dir = os.path.join(parent_dir, "test")
    #     if test_dir not in sys.path:
    #         sys.path.insert(0, test_dir)
        
    #     from test_fuction.get_daily_stats import get_daily_stats
    #     executor_manager.register_tool("get_daily_stats", get_daily_stats)
    #     print("âœ… Registered tool: get_daily_stats")
    # except Exception as e:
    #     print(f"âš ï¸ Warning: Could not load get_daily_stats tool: {e}")
    
    # Setup LLM factory (using environment variable or defaults)
    setup_llm_factory()
    
    yield
    
    # å…³é—­æ—¶çš„æ¸…ç†
    print("ğŸ›‘ Backend API shutting down...")
    executor_manager.executors.clear()


app = FastAPI(
    title="Simple LLM Playground API",
    description="Backend API for LLM Executor debugging and visualization",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =============================================================================
# API ç«¯ç‚¹
# =============================================================================

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - å¥åº·æ£€æŸ¥"""
    return {
        "status": "running",
        "service": "Simple LLM Playground API",
        "version": "1.0.0"
    }


@app.get("/api/tools")
async def list_tools():
    """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„å·¥å…·"""
    tools = []
    for name, tool in executor_manager._tools_registry.items():
        tool_data = {
            "name": name,
            "description": getattr(tool, 'description', 'No description'),
        }
        
        # Try to extract parameter information from langchain tool
        try:
            # Check if it's a langchain tool with args_schema
            if hasattr(tool, 'args_schema'):
                schema = tool.args_schema
                if schema:
                    # Get field information from pydantic model
                    tool_data["parameters"] = {}
                    if hasattr(schema, 'model_fields'):
                        for field_name, field_info in schema.model_fields.items():
                            tool_data["parameters"][field_name] = {
                                "type": str(field_info.annotation),
                                "required": field_info.is_required(),
                                "description": field_info.description or ""
                            }
            
            # Also try to get from function signature
            if hasattr(tool, 'func'):
                import inspect
                sig = inspect.signature(tool.func)
                if "parameters" not in tool_data:
                    tool_data["parameters"] = {}
                
                for param_name, param in sig.parameters.items():
                    if param_name not in tool_data["parameters"]:
                        tool_data["parameters"][param_name] = {
                            "type": str(param.annotation) if param.annotation != inspect.Parameter.empty else "Any",
                            "required": param.default == inspect.Parameter.empty,
                            "description": ""
                        }
        except Exception as e:
            # If extraction fails, just skip parameters
            print(f"Warning: Could not extract parameters for tool {name}: {e}")
        
        tools.append(tool_data)
    
    return {"tools": tools}



@app.post("/api/executor/init", response_model=InitExecutorResponse)
async def init_executor(request: InitExecutorRequest):
    """
    åˆå§‹åŒ–æ‰§è¡Œå™¨

    åˆ›å»ºä¸€ä¸ªæ–°çš„ AsyncExecutor å®ä¾‹ï¼Œå‡†å¤‡æ‰§è¡Œè®¡åˆ’
    """
    try:
        # è§£æ ExecutionPlan
        plan = ExecutionPlan(**request.plan)
        if request.default_tool_limit is None:
            request.default_tool_limit = 1
        # åˆ›å»ºæ‰§è¡Œå™¨
        executor_id = executor_manager.create_executor(
            plan=plan,
            user_message=request.user_message,
            default_tools_limit=request.default_tool_limit # å½“è¿™ä¸ªæ˜¯Noneæ—¶ï¼Œå¯¼è‡´åé¢ä¼šæŠ¥é”™
        )
        
        return InitExecutorResponse(
            executor_id=executor_id,
            status="initialized",
            node_count=len(plan.nodes),
            message=f"Executor initialized with {len(plan.nodes)} nodes"
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/api/executor/{executor_id}/run", response_model=ExecutionResultResponse)
async def run_executor(executor_id: str, background_tasks: BackgroundTasks):
    """
    è¿è¡Œæ‰§è¡Œå™¨ï¼ˆæ‰§è¡Œæ•´ä¸ªè®¡åˆ’ï¼‰
    
    åœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œï¼Œç«‹å³è¿”å›
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    # æ›´æ–°çŠ¶æ€
    executor_manager.executor_status[executor_id] = "running"
    
    async def execute_in_background():
        try:
            result = await executor.execute()
            executor_manager.executor_status[executor_id] = "completed"
        except Exception as e:
            executor_manager.executor_status[executor_id] = f"failed: {str(e)}"
    
    # æ·»åŠ åå°ä»»åŠ¡
    background_tasks.add_task(execute_in_background)
    
    return ExecutionResultResponse(
        executor_id=executor_id,
        status="running",
        content=None,
        tokens_usage=executor.tokens_usage,
        message="Execution started in background"
    )


@app.post("/api/executor/{executor_id}/run-sync", response_model=ExecutionResultResponse)
async def run_executor_sync(executor_id: str):
    """
    åŒæ­¥è¿è¡Œæ‰§è¡Œå™¨ï¼ˆç­‰å¾…æ‰§è¡Œå®Œæˆï¼‰
    
    ç›´æ¥æ‰§è¡Œå¹¶è¿”å›ç»“æœï¼Œé€‚ç”¨äºéœ€è¦ç«‹å³è·å–ç»“æœçš„åœºæ™¯
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    # æ›´æ–°çŠ¶æ€
    executor_manager.executor_status[executor_id] = "running"
    
    try:
        result = await executor.execute()
        executor_manager.executor_status[executor_id] = "completed"
        
        return ExecutionResultResponse(
            executor_id=executor_id,
            status="completed",
            content=result.get("content"),
            tokens_usage=result.get("tokens_usage", {}),
            message="Execution completed"
        )
    except Exception as e:
        executor_manager.executor_status[executor_id] = "failed"
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/executor/{executor_id}/step")
async def step_executor(executor_id: str, request: StepExecutorRequest = None):
    """
    å•æ­¥æ‰§è¡Œ
    
    æ‰§è¡Œä¸‹ä¸€ä¸ªå¾…æ‰§è¡Œçš„èŠ‚ç‚¹ï¼Œè¿”å›èŠ‚ç‚¹ä¸Šä¸‹æ–‡
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    try:
        # æ‰§è¡Œå•æ­¥
        context = await executor.execute_step()
        
        if context is None:
            return {
                "status": "completed",
                "message": "All nodes have been executed",
                "node_context": None,
                "progress": executor.get_execution_progress()
            }
        
        return {
            "status": "success",
            "message": f"Node {context.node_id} executed",
            "node_context": context.model_dump(),
            "progress": executor.get_execution_progress()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/executor/{executor_id}/status", response_model=ExecutorStatusResponse)
async def get_executor_status(executor_id: str):
    """
    è·å–æ‰§è¡Œå™¨çŠ¶æ€
    
    è¿”å›æ•´ä½“çŠ¶æ€å’Œæ‰€æœ‰èŠ‚ç‚¹çš„æ‰§è¡ŒçŠ¶æ€
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    overall_status = executor_manager.executor_status.get(executor_id, "unknown")
    
    return ExecutorStatusResponse(
        executor_id=executor_id,
        overall_status=overall_status,
        progress=executor.get_execution_progress(),
        node_states=[s.model_dump() for s in executor.get_all_node_states()]
    )


@app.get("/api/executor/{executor_id}/nodes/{node_id}/context")
async def get_node_context(executor_id: str, node_id: int):
    """
    è·å–èŠ‚ç‚¹ä¸Šä¸‹æ–‡
    
    è¿”å›æŒ‡å®šèŠ‚ç‚¹çš„è¯¦ç»†æ‰§è¡Œä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    context = executor.get_node_context(node_id)
    if not context:
        raise HTTPException(status_code=404, detail=f"Context for node {node_id} not found")
    
    return context.model_dump()


@app.get("/api/executor/{executor_id}/messages")
async def get_executor_messages(executor_id: str, thread_id: str = None):
    """
    è·å–æ‰§è¡Œå™¨çš„æ¶ˆæ¯
    
    å¯é€‰æŒ‡å®š thread_id è·å–ç‰¹å®šçº¿ç¨‹çš„æ¶ˆæ¯
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    if thread_id:
        messages = executor._get_thread_messages(thread_id)
        return {
            "thread_id": thread_id,
            "messages": executor._serialize_messages(messages)
        }
    else:
        # è¿”å›æ‰€æœ‰çº¿ç¨‹çš„æ¶ˆæ¯
        all_messages = {}
        for tid in executor.context["messages"]:
            all_messages[tid] = executor._serialize_messages(
                executor.context["messages"][tid]
            )
        return {"threads": all_messages}


@app.delete("/api/executor/{executor_id}")
async def terminate_executor(executor_id: str):
    """
    ç»ˆæ­¢å¹¶åˆ é™¤æ‰§è¡Œå™¨
    """
    executor = executor_manager.get_executor(executor_id)
    if not executor:
        raise HTTPException(status_code=404, detail="Executor not found")
    
    executor_manager.remove_executor(executor_id)
    
    return {
        "status": "terminated",
        "message": f"Executor {executor_id} has been terminated"
    }


@app.get("/api/executors")
async def list_executors():
    """
    åˆ—å‡ºæ‰€æœ‰æ‰§è¡Œå™¨
    """
    executors = []
    for eid, executor in executor_manager.executors.items():
        executors.append({
            "executor_id": eid,
            "status": executor_manager.executor_status.get(eid, "unknown"),
            "progress": executor.get_execution_progress()
        })
    return {"executors": executors}


# =============================================================================
# å·¥å…·æ³¨å†Œ APIï¼ˆç”¨äºåŠ¨æ€æ³¨å†Œå·¥å…·ï¼‰
# =============================================================================

@app.post("/api/tools/register")
async def register_tool_endpoint(
    tool_name: str,
    tool_module: str,
    tool_function: str,
    limit: int = 10
):
    """
    åŠ¨æ€æ³¨å†Œå·¥å…·
    
    ä»æŒ‡å®šæ¨¡å—å¯¼å…¥å·¥å…·å‡½æ•°å¹¶æ³¨å†Œ
    """
    try:
        import importlib
        module = importlib.import_module(tool_module)
        tool_func = getattr(module, tool_function)
        
        executor_manager.register_tool(tool_name, tool_func)
        
        return {
            "status": "success",
            "message": f"Tool '{tool_name}' registered successfully"
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


# =============================================================================
# ç”¨äºæµ‹è¯•çš„è¾…åŠ©å‡½æ•°
# =============================================================================

def setup_test_tools():
    """è®¾ç½®æµ‹è¯•å·¥å…·ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰"""
    from langchain_core.tools import tool
    
    @tool
    def add(a: int, b: int) -> int:
        """Add two numbers"""
        return a + b
    
    @tool
    def multiply(a: int, b: int) -> int:
        """Multiply two numbers"""
        return a * b
    
    executor_manager.register_tool("add", add)
    executor_manager.register_tool("multiply", multiply)


def setup_llm_factory(
    api_key: str = None,
    model: str = "qwen-plus-2025-12-01",
    api_base: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
    **kwargs
):
    """
    è®¾ç½® LLM å·¥å‚å‡½æ•°

    æ”¯æŒé˜¿é‡Œäº‘ DashScope API (é€šä¹‰åƒé—®) å’Œ OpenAI API

    Args:
        api_key: APIå¯†é’¥ (DashScope API Key æˆ– OpenAI API Key)ã€‚å¦‚æœä¸ä¼ ï¼Œå°è¯•ä»ç¯å¢ƒè¯»å–ã€‚
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ "qwen-plus"
            - é€šä¹‰åƒé—®: "qwen-plus", "qwen-max", "qwen-turbo" ç­‰
            - OpenAI: "gpt-4", "gpt-3.5-turbo" ç­‰
        api_base: APIåŸºç¡€URL
            - é˜¿é‡Œäº‘: "https://dashscope.aliyuncs.com/compatible-mode/v1"
            - OpenAI: "https://api.openai.com/v1" (é»˜è®¤)
        **kwargs: å…¶ä»–å‚æ•°å¦‚ temperature, top_p ç­‰
    """
    # å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å– API Key
    if not api_key:
        api_key = os.getenv("DASHSCOPE_API_KEY") or os.getenv("OPENAI_API_KEY")

    if not api_key:
        print("âš ï¸ Warning: No API key found. Please set DASHSCOPE_API_KEY or OPENAI_API_KEY environment variable.")

    # ä½¿ç”¨ lambda æ•è·æ‰€æœ‰å‚æ•°ï¼Œç¡®ä¿é—­åŒ…æ­£ç¡®æ•è·å˜é‡
    factory = lambda: _create_llm_instance(
        model=model,
        api_key=api_key,
        api_base=api_base,
        **kwargs
    )

    executor_manager.set_llm_factory(factory)


def _create_llm_instance(
    model: str,
    api_key: str,
    api_base: str,
    **kwargs
):
    """
    åˆ›å»º LLM å®ä¾‹çš„è¾…åŠ©å‡½æ•°

    Args:
        model: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        api_base: APIåŸºç¡€URL
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        ChatOpenAI å®ä¾‹
    """
    try:
        from langchain_openai import ChatOpenAI

        # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼ï¼Œæ”¯æŒé˜¿é‡Œäº‘ DashScope å’Œ OpenAI
        return ChatOpenAI(
            model=model,
            openai_api_key=api_key,
            openai_api_base=api_base,
            temperature=kwargs.get('temperature', 0.7),
            top_p=kwargs.get('top_p', 0.9)
        )
    except ImportError:
        raise ValueError("langchain_openai not installed. Run: pip install langchain-openai")


# =============================================================================
# å¯åŠ¨å…¥å£
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    
    # 1. è®¾ç½® LLM å·¥å‚ (ä¼˜å…ˆ)
    setup_llm_factory()
    
    # 2. è®¾ç½®æµ‹è¯•å·¥å…·
    # setup_test_tools()
    # æµ‹è¯• LLM  é“¾æ¥
    # llm = executor_manager._llm_factory()
    # print(llm.invoke("Hello, how are you?"))
    # Try to import global config from main.py
    try:
        import config
        port = getattr(config, "BACKEND_PORT", 8001)
        print(f"âš™ï¸  Loaded configuration from main.py: Port {port}")
    except ImportError:
        port = 8001
        print("âš ï¸  Warning: Could not import BACKEND_PORT from main.py, using default 8001")

    print("ğŸš€ Starting Simple LLM Playground API...")
    print(f"ğŸ“ API docs available at: http://localhost:{port}/docs")

    uvicorn.run(app, host="0.0.0.0", port=port)
